#Below sample yaml is used to schedule pods on 2 A100 GPUs

apiVersion: v1
kind: Pod
metadata:
 name: a100-sb4j-codet5-base-high-reptile-20-step
 namespace: sfr-ns-weishi-wang
spec:
  restartPolicy: OnFailure
  volumes:
    - name: sfr-home-pv-weishi-wang
      persistentVolumeClaim:
        claimName: sfr-home-pvc-weishi-wang
    - name: sfr-share-pv-weishi-wang
      persistentVolumeClaim:
        claimName: sfr-share-pvc-weishi-wang
    - name: dshm
      emptyDir:
        medium: Memory
  containers:
    - name: weishi-env-a100
      image: "gcr.io/salesforce-research-internal/weishi-a100-env"
      command: ["/bin/bash", "-c", "cd /export/home/share/repair_CodeT5/sh && export PATH=/export/home/anaconda3/bin:$PATH && source activate meta-a100 && python run_exp.py --do_meta_train_crossfit --do_reptile --model_tag codet5_base --task sb4j_high_resource_meta_crossfit --args.task_batch_size 20"]
      resources:
        limits:
          nvidia.com/gpu: 1
          cpu: "11"
          memory: 75G
      volumeMounts:
        - name: sfr-home-pv-weishi-wang
          mountPath: "/export/home"
        - name: sfr-share-pv-weishi-wang
          mountPath: "/export/share"
        - name: dshm
          mountPath: "/dev/shm"
  nodeSelector:
    cloud.google.com/gke-accelerator: nvidia-tesla-a100
  tolerations:
  - key: "gpu_num"
    operator: "Equal"
    value: "2"
    effect: "NoSchedule"

